{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Start_Datetime_SINE</th>\n",
       "      <th>Start_Datetime_COSINE</th>\n",
       "      <th>End_Datetime_SINE</th>\n",
       "      <th>End_Datetime_COSINE</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DPH</th>\n",
       "      <th>TEPH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>340.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.397059</td>\n",
       "      <td>0.284093</td>\n",
       "      <td>-0.595033</td>\n",
       "      <td>0.283465</td>\n",
       "      <td>-0.595847</td>\n",
       "      <td>2.623907</td>\n",
       "      <td>2.053294</td>\n",
       "      <td>17.180379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.766116</td>\n",
       "      <td>0.630999</td>\n",
       "      <td>0.410756</td>\n",
       "      <td>0.631107</td>\n",
       "      <td>0.409840</td>\n",
       "      <td>1.867296</td>\n",
       "      <td>0.660985</td>\n",
       "      <td>6.346039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.996916</td>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-0.996990</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.225284</td>\n",
       "      <td>-0.958547</td>\n",
       "      <td>-0.224593</td>\n",
       "      <td>-0.959161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.675000</td>\n",
       "      <td>13.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.324880</td>\n",
       "      <td>-0.731646</td>\n",
       "      <td>0.323944</td>\n",
       "      <td>-0.729693</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>17.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.921236</td>\n",
       "      <td>-0.330484</td>\n",
       "      <td>0.920311</td>\n",
       "      <td>-0.331887</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.390000</td>\n",
       "      <td>20.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.398202</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.395138</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Location  Start_Datetime_SINE  Start_Datetime_COSINE  \\\n",
       "count  340.000000           343.000000             343.000000   \n",
       "mean     2.397059             0.284093              -0.595033   \n",
       "std      1.766116             0.630999               0.410756   \n",
       "min      1.000000            -0.996916              -0.999993   \n",
       "25%      1.000000            -0.225284              -0.958547   \n",
       "50%      2.000000             0.324880              -0.731646   \n",
       "75%      3.000000             0.921236              -0.330484   \n",
       "max      7.000000             1.000000               0.398202   \n",
       "\n",
       "       End_Datetime_SINE  End_Datetime_COSINE   DayOfWeek         DPH  \\\n",
       "count         343.000000           343.000000  343.000000  343.000000   \n",
       "mean            0.283465            -0.595847    2.623907    2.053294   \n",
       "std             0.631107             0.409840    1.867296    0.660985   \n",
       "min            -0.996990            -0.999999    0.000000    0.250000   \n",
       "25%            -0.224593            -0.959161    1.000000    1.675000   \n",
       "50%             0.323944            -0.729693    3.000000    2.070000   \n",
       "75%             0.920311            -0.331887    4.000000    2.390000   \n",
       "max             0.999980             0.395138    6.000000    6.000000   \n",
       "\n",
       "             TEPH  \n",
       "count  343.000000  \n",
       "mean    17.180379  \n",
       "std      6.346039  \n",
       "min      1.720000  \n",
       "25%     13.200000  \n",
       "50%     17.020000  \n",
       "75%     20.930000  \n",
       "max     45.000000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "SECONDS_IN_YEAR = (dt.datetime(2021, 1, 1) - dt.datetime(2020, 1, 1)).total_seconds()\n",
    "\n",
    "def get_population_density(location):\n",
    "    if location == \"Kenosha;WI\":\n",
    "        return 1\n",
    "    elif location == \"Hackensack;NJ\":\n",
    "        return 2\n",
    "    elif location == \"St. Louis;MO\":\n",
    "        return 3\n",
    "    elif location == \"Ellicott City;MD\":\n",
    "        return 4\n",
    "    elif location == \"Baltimore;MD\":\n",
    "        return 5\n",
    "    elif location == \"Catonsville;MD\":\n",
    "        return 6\n",
    "    elif location == \"North Bethesda;MD\":\n",
    "        return 7\n",
    "    elif location == \"Los Angeles;CA\":\n",
    "        return 8\n",
    "\n",
    "def get_SINE(date):\n",
    "    seconds = (date - dt.datetime(2020, 1, 1, 0)).total_seconds()\n",
    "    return np.sin(2 * np.pi * seconds / SECONDS_IN_YEAR)\n",
    "\n",
    "def get_COSINE(date):\n",
    "    seconds = (date - dt.datetime(2020, 1, 1, 0)).total_seconds()\n",
    "    return np.cos(2 * np.pi * seconds / SECONDS_IN_YEAR)\n",
    "\n",
    "def get_hour_difference(start_datetime, end_datetime):\n",
    "    return (end_datetime - start_datetime).total_seconds() / 3600\n",
    "\n",
    "# Import file\n",
    "#import_file = \"DoorDash_Kenosha_WI_V2.csv\"\n",
    "#import_file = \"DoorDash_Hackensack_NJ_V2.csv\"\n",
    "#import_file = \"DoorDash_St._Louis_MO_V2.csv\"\n",
    "#import_file = \"DoorDash_MD_V2.csv\"\n",
    "#import_file = \"DoorDash_Los_Angeles_CA_V2.csv\"\n",
    "import_file = \"DoorDash_Combined_V2.csv\"\n",
    "#columns = [\"Location\", \"Start_Datetime\", \"End_Datetime\", \"DPH\", \"TEPH\"]\n",
    "columns = [\"Start_Datetime\", \"Deliveries\", \"Location\", \"Total_Earnings\", \"DPH\", \"ID\", \"End_Datetime\", \"TEPH\"]\n",
    "df = pd.read_csv(import_file, names=columns)\n",
    "\n",
    "df.describe()\n",
    "\n",
    "# Change to numerical data\n",
    "df[\"Location\"] = df[\"Location\"].map(get_population_density)\n",
    "df[\"Start_Datetime\"] = pd.to_datetime(df[\"Start_Datetime\"], format=\"%Y-%m-%dT%H:%MZ\", errors=\"coerce\")\n",
    "df[\"DayOfWeek\"] = df[\"Start_Datetime\"].dt.dayofweek\n",
    "df[\"Start_Datetime\"] = (df[\"Start_Datetime\"] - dt.datetime(2020, 1, 1)).dt.total_seconds()\n",
    "df[\"Start_Datetime_SINE\"] = np.sin(2 * np.pi * df[\"Start_Datetime\"] / SECONDS_IN_YEAR)\n",
    "df[\"Start_Datetime_COSINE\"] = np.cos(2 * np.pi * df[\"Start_Datetime\"] / SECONDS_IN_YEAR)\n",
    "df[\"End_Datetime\"] = pd.to_datetime(df[\"End_Datetime\"], format=\"%Y-%m-%dT%H:%MZ\", errors=\"coerce\")\n",
    "df[\"End_Datetime\"] = (df[\"End_Datetime\"] - dt.datetime(2020, 1, 1)).dt.total_seconds()\n",
    "df[\"End_Datetime_SINE\"] = np.sin(2 * np.pi * df[\"End_Datetime\"] / SECONDS_IN_YEAR)\n",
    "df[\"End_Datetime_COSINE\"] = np.cos(2 * np.pi * df[\"End_Datetime\"] / SECONDS_IN_YEAR)\n",
    "df = df[[\"Location\", \"Start_Datetime_SINE\", \"Start_Datetime_COSINE\", \"End_Datetime_SINE\", \"End_Datetime_COSINE\", \"DayOfWeek\", \"DPH\", \"TEPH\"]]\n",
    "\n",
    "# Separate features and classes\n",
    "feature_names = [\"Location\", \"Start_Datetime_SINE\", \"Start_Datetime_COSINE\", \"End_Datetime_SINE\", \"End_Datetime_COSINE\", \"DayOfWeek\"]\n",
    "all_features = df[feature_names].values\n",
    "all_classes_TEPH = df[\"TEPH\"].values\n",
    "\n",
    "# Normalize data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "all_features_scaled = scaler.fit_transform(all_features)\n",
    "\n",
    "# Test inputs\n",
    "location = get_population_density(\"Kenosha;WI\")\n",
    "start_datetime = dt.datetime(2020, 10, 5, 15, 0)\n",
    "end_datetime = dt.datetime(2020, 10, 5, 16, 0)\n",
    "start_datetime_SINE = get_SINE(start_datetime)\n",
    "start_datetime_COSINE = get_COSINE(start_datetime)\n",
    "end_datetime_SINE = get_SINE(end_datetime)\n",
    "end_datetime_COSINE = get_COSINE(end_datetime)\n",
    "dayOfWeek = start_datetime.weekday()\n",
    "hour_difference = get_hour_difference(start_datetime, end_datetime)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEPH Calculation: 17.180379008746357\n",
      "TEPH STD Calculation: 6.3460392977205125\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "TEPH_mean = df[\"TEPH\"].mean()\n",
    "print(\"TEPH Calculation:\", TEPH_mean * hour_difference)\n",
    "\n",
    "TEPH_std = df[\"TEPH\"].std()\n",
    "TEPH_variance = TEPH_std * TEPH_std\n",
    "TEPH_test_std = math.sqrt(TEPH_variance * hour_difference)\n",
    "print(\"TEPH STD Calculation:\", TEPH_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Random Forest</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a3bdf6727ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTEPH_array\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mestimates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_datetime_SINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_datetime_COSINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_datetime_SINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_datetime_COSINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayOfWeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-a3bdf6727ebd>\u001b[0m in \u001b[0;36mget_random_forest\u001b[0;34m(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_features_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_classes_TEPH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mTEPH_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_classes_TEPH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mTEPH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mTEPH_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTEPH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEPH_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TEPH\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=10, random_state=1)\n",
    "\n",
    "# Return random forest estimates and k-fold cross-validation scores\n",
    "def get_random_forest(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek):\n",
    "    test_input = [[location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek]]\n",
    "    # TEPH\n",
    "    cv_scores = cross_val_score(clf, all_features_scaled, all_classes_TEPH, cv=10)\n",
    "    TEPH_k = cv_scores.mean()\n",
    "    clf.fit(all_features_scaled, all_classes_TEPH)\n",
    "    TEPH = clf.predict(test_input)[0]\n",
    "    TEPH_array = [TEPH, TEPH_k, \"TEPH\"]\n",
    "    return [TEPH_array]\n",
    "\n",
    "estimates = get_random_forest(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek)\n",
    "print(estimates)\n",
    "\n",
    "print(\"TEPH Calculation:\", estimates[0][0] * hour_difference)\n",
    "TEPH_test_std = math.sqrt(TEPH_variance * hour_difference)\n",
    "print(\"TEPH STD Calculation:\", TEPH_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Linear SVM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.62761104066157, 0.00018257027584018816, 'TEPH']]\n",
      "TEPH Calculation: 14.62761104066157\n",
      "TEPH STD Calculation: 6.178973095657914\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Hyperparameters\n",
    "C = 1.0\n",
    "\n",
    "svr = svm.SVR(kernel=\"linear\", C=C)\n",
    "\n",
    "# Return linear SVM estimates and k-fold cross-validation scores\n",
    "def get_linear_SVM(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek):\n",
    "    test_input = [[location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek]]\n",
    "    # TEPH\n",
    "    cv_scores = cross_val_score(svr, all_features_scaled, all_classes_TEPH, cv=10)\n",
    "    TEPH_k = cv_scores.mean()\n",
    "    svr.fit(all_features_scaled, all_classes_TEPH)\n",
    "    TEPH = svr.predict(test_input)[0]\n",
    "    TEPH_array = [TEPH, TEPH_k, \"TEPH\"]\n",
    "    return [TEPH_array]\n",
    "\n",
    "estimates = get_linear_SVM(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek)\n",
    "print(estimates)\n",
    "\n",
    "print(\"TEPH Calculation:\", estimates[0][0] * hour_difference)\n",
    "TEPH_test_std = math.sqrt(TEPH_variance * hour_difference)\n",
    "print(\"TEPH STD Calculation:\", TEPH_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>RBF SVM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.25249904389894, 0.03852328932240066, 'TEPH']]\n",
      "TEPH Calculation: 14.25249904389894\n",
      "TEPH STD Calculation: 6.178973095657914\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Hyperparameters\n",
    "C = 1.0\n",
    "\n",
    "svr = svm.SVR(kernel=\"rbf\", C=C)\n",
    "\n",
    "# Return RBF SVM estimates and k-fold cross-validation scores\n",
    "def get_RBF_SVM(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek):\n",
    "    test_input = [[location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek]]\n",
    "    # TEPH\n",
    "    cv_scores = cross_val_score(svr, all_features_scaled, all_classes_TEPH, cv=10)\n",
    "    TEPH_k = cv_scores.mean()\n",
    "    svr.fit(all_features_scaled, all_classes_TEPH)\n",
    "    TEPH = svr.predict(test_input)[0]\n",
    "    TEPH_array = [TEPH, TEPH_k, \"TEPH\"]\n",
    "    return [TEPH_array]\n",
    "\n",
    "estimates = get_RBF_SVM(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek)\n",
    "print(estimates)\n",
    "\n",
    "print(\"TEPH Calculation:\", estimates[0][0] * hour_difference)\n",
    "TEPH_test_std = math.sqrt(TEPH_variance * hour_difference)\n",
    "print(\"TEPH STD Calculation:\", TEPH_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>K-Nearest Neighbors</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.425999999999998, -0.021429989476138177, 'TEPH']]\n",
      "TEPH Calculation: 13.425999999999998\n",
      "TEPH STD Calculation: 6.178973095657914\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "clf = neighbors.KNeighborsRegressor(n_neighbors=10)\n",
    "\n",
    "# Find optimal value for neighbors\n",
    "#for n in range(1, 50):\n",
    "#    clf = neighbors.KNeighborsRegressor(n_neighbors=n)\n",
    "#    cv_scores = cross_val_score(clf, all_features_scaled, all_classes_TEPH, cv=10)\n",
    "#    print(n, cv_scores.mean())\n",
    "\n",
    "# Return KNN estimates and k-fold cross-validation scores\n",
    "def get_KNN(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek):\n",
    "    test_input = [[location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek]]\n",
    "    # TEPH\n",
    "    cv_scores = cross_val_score(clf, all_features_scaled, all_classes_TEPH, cv=10)\n",
    "    TEPH_k = cv_scores.mean()\n",
    "    clf.fit(all_features_scaled, all_classes_TEPH)\n",
    "    TEPH = clf.predict(test_input)[0]\n",
    "    TEPH_array = [TEPH, TEPH_k, \"TEPH\"]\n",
    "    return [TEPH_array]\n",
    "\n",
    "estimates = get_KNN(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek)\n",
    "print(estimates)\n",
    "\n",
    "print(\"TEPH Calculation:\", estimates[0][0] * hour_difference)\n",
    "TEPH_test_std = math.sqrt(TEPH_variance * hour_difference)\n",
    "print(\"TEPH STD Calculation:\", TEPH_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Create model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(6, input_dim=6, kernel_initializer=\"normal\", activation=\"relu\"))\n",
    "    model.add(Dense(3, kernel_initializer=\"normal\", activation=\"relu\"))\n",
    "    model.add(Dense(1, kernel_initializer=\"normal\", activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: A target array with shape (271, 1) was passed for an output of shape (None, 3) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/joshchoi/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: A target array with shape (272, 1) was passed for an output of shape (None, 3) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (302, 1) was passed for an output of shape (None, 3) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-3ddd7038bcb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTEPH_array\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mestimates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_datetime_SINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_datetime_COSINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_datetime_SINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_datetime_COSINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayOfWeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-3ddd7038bcb4>\u001b[0m in \u001b[0;36mget_neural_network\u001b[0;34m(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_features_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_classes_TEPH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mTEPH_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_classes_TEPH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mTEPH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mTEPH_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTEPH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEPH_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TEPH\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (302, 1) was passed for an output of shape (None, 3) while using as loss `binary_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "estimator = KerasRegressor(build_fn=create_model, epochs=100, verbose=0)\n",
    "\n",
    "# Return neural network estimates and k-fold cross-validation scores\n",
    "def get_neural_network(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek):\n",
    "    test_input = [[location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek]]\n",
    "    # TEPH\n",
    "    cv_scores = cross_val_score(estimator, all_features_scaled, all_classes_TEPH, cv=10)\n",
    "    TEPH_k = cv_scores.mean()\n",
    "    estimator.fit(all_features_scaled, all_classes_TEPH)\n",
    "    TEPH = estimator.predict(test_input)\n",
    "    TEPH_array = [TEPH, TEPH_k, \"TEPH\"]\n",
    "    return [TEPH_array]\n",
    "\n",
    "estimates = get_neural_network(location, start_datetime_SINE, start_datetime_COSINE, end_datetime_SINE, end_datetime_COSINE, dayOfWeek)\n",
    "print(estimates)\n",
    "\n",
    "print(\"TEPH Calculation:\", estimates[0][0] * hour_difference)\n",
    "TEPH_test_std = math.sqrt(TEPH_variance * hour_difference)\n",
    "print(\"TEPH STD Calculation:\", TEPH_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
